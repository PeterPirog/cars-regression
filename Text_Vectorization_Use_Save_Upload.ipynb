{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Vectorization Use Save Upload.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeterPirog/cars-regression/blob/main/Text_Vectorization_Use_Save_Upload.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w47A5xPdXp1W"
      },
      "source": [
        "# KERAS TEXT VECTORIZATION LAYER: USE, SAVE, AND UPLOAD\n",
        "\n",
        "**Author:** [Murat Karakaya](https://www.linkedin.com/in/muratkarakaya/)<br>\n",
        "**Date created:** 05 Oct 2021<br>\n",
        "**Last modified:** 24 Oct 2021<br>\n",
        "**Description:** This is a new part of the \"**[tf.keras.layers: Understand & Use](https://www.youtube.com/playlist?list=PLQflnv_s49v_7WIgOo9mVKptLZHyOYysD)**\" / \"[**tf.keras.layers: Anla ve Kullan**](https://www.youtube.com/playlist?list=PLQflnv_s49v9h85zD1_GDfTxZOrCWTDhp)\" series. In this part, we will build, adapt, use, save, and upload the Keras TextVectorization layer. \n",
        "\n",
        "We will download a [Kaggle Dataset](https://www.kaggle.com/savasy/multiclass-classification-data-for-turkish-tc32?select=ticaret-yorum.csv) in which there are 32 topics and more than 400K total reviews. \n",
        "In this tutorial, we will use this dataset for a multi class text classification task.\n",
        "\n",
        "Our **main aim** is to learn how to efectively use the Keras `TextVectorization` layer in practice.\n",
        "\n",
        "The tutorial has 5 parts:\n",
        "\n",
        "* **PART A: BACKGROUND**\n",
        "* **PART B: KNOW THE DATA**\n",
        "* **PART C: USE KERAS TEXT VECTORIZATION LAYER**\n",
        "* **PART D: BUILD AN END-TO-END MODEL**\n",
        "* **PART E: SUMMARY**\n",
        "\n",
        "\n",
        "At the end of this tutorial, we will cover:\n",
        "* What a Keras `TextVectorization` layer is\n",
        "* Why we need to use a Keras `TextVectorization` layer in Natural Languge Processing (NLP) tasks\n",
        "* How to employ a Keras `TextVectorization` layer in **Text Preprocessing**\n",
        "* How to integrate a Keras `TextVectorization` layer to a trained model\n",
        "* How to save and upload a Keras `TextVectorization` layer and a model with a Keras `TextVectorization` layer\n",
        "* How to integrate a Keras `TextVectorization` layer with **TensorFlow Data Pipeline** API (`tf.data`)\n",
        "* How to design, train, save, and load an End-to-End model using Keras `TextVectorization` layer\n",
        "\n",
        "**Accessible on:**\n",
        "* [YouTube in English](https://youtube.com/playlist?list=PLQflnv_s49v8Eo2idw9Ju5Qq3JTEF-OFW)\n",
        "* [YouTube in Turkish](https://youtube.com/playlist?list=PLQflnv_s49v8-xeTLx1QmuE-YkRB4bToF)\n",
        "* [Medium](https://kmkarakaya.medium.com/text-vectorization-use-save-upload-54d65945d222)\n",
        "* [Github pages](https://kmkarakaya.github.io/Deep-Learning-Tutorials/)\n",
        "* [Github Repo](https://github.com/kmkarakaya/Deep-Learning-Tutorials)\n",
        "* [Google Colab](https://colab.research.google.com/drive/1_hiUXcX6DwGEsPP2iE7i-HAs-5HqQrSe?usp=sharing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTmjxD3k10sj"
      },
      "source": [
        "# REFERENCES\n",
        "* [Keras Preprocessing layers by Keras.io](https://keras.io/api/layers/preprocessing_layers/)\n",
        "* [Text classification from scratch by Keras.io](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
        "* [TextVectorization layer by Keras.io](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7NHPGuvcdm3"
      },
      "source": [
        "# **PART A: BACKGROUND**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXnOwUb7oO0-"
      },
      "source": [
        "# 1 TERMS & CONCEPTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3wsbjvlW5bk"
      },
      "source": [
        "## 1.1 What is Text Vectorization?\n",
        "\n",
        "Text Vectorization is the process of converting text into numerical representation. \n",
        "\n",
        "There are many different techniques proposed to convert text to a numerical form such as:\n",
        "* One-hot Encoding (OHE)\n",
        "* Count Vectorizer\n",
        "* Bag-of-Words (BOW)\n",
        "* N-grams\n",
        "* Term Frequency\n",
        "* Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "* Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4cW26J5YnIR"
      },
      "source": [
        "## 1.2. What is Text Preprocessing?\n",
        "Text preprocessing is traditionally an important step for natural language processing (NLP) tasks. It transforms text into a more suitable form so that Machine Learning or Deep Learning algorithms can perform better.\n",
        "\n",
        "The main phases of Text preprocessing:\n",
        "* **Noise Removal** (cleaning) – Removing unnecessary characters and formatting\n",
        "* **Tokenization** – break multi-word strings into smaller components\n",
        "* **Normalization** – a catch-all term for processing data; this includes stemming and lemmatization\n",
        "\n",
        "\n",
        "Some of the common **Noise Removal** (cleaning) steps are:\n",
        "\n",
        "* Removal of Punctuations\n",
        "* Removal of Frequent words\n",
        "* Removal of Rare words\n",
        "* Removal of emojis\n",
        "* Removal of emoticons\n",
        "* Conversion of emoticons to words\n",
        "* Conversion of emojis to words\n",
        "* Removal of URLs\n",
        "* Removal of HTML tags\n",
        "* Chat words conversion\n",
        "* Spelling correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6LfU5Muatns"
      },
      "source": [
        "**Tokenization** is about splitting strings of text into smaller pieces, or “tokens”. Paragraphs can be tokenized into sentences and sentences can be tokenized into words. \n",
        "\n",
        "\n",
        "**Noise Removal** and **Tokenization** and  are staples of almost all text pre-processing pipelines. However, some data may require further processing through text **normalization**. Some of the common **normalization** steps are:\n",
        "* Upper or lowercasing\n",
        "* Stopword removal\n",
        "* Stemming – bluntly removing prefixes and suffixes from a word\n",
        "* Lemmatization – replacing a single-word token with its root\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MIVN5X4Di96"
      },
      "source": [
        "## 1.3. What is Keras Text Vectorization layer?\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKTqWoHWD0_a"
      },
      "source": [
        "`tf.keras.layers.TextVectorization` layer is one of the [Keras Preprocessing layers](https://keras.io/guides/preprocessing_layers/). \n",
        "\n",
        "We can preproces the input by using different libraries such as Python String library, or SciKit Learn library, etc. \n",
        "\n",
        "However, there are very important advantages using the [Keras Preprocessing layers](https://keras.io/guides/preprocessing_layers/):\n",
        "\n",
        "* You can build **Keras-native** input processing **pipelines**. These input processing pipelines can be used as **independent** preprocessing code in **non-Keras workflows**, combined directly with Keras models, and exported as part of a Keras SavedModel.\n",
        "\n",
        "* You can build and **export** models that are **truly end-to-end**: models that accept **raw data** (images or raw structured data) as input; models that handle feature **normalization** or feature value **indexing** on their own.\n",
        "\n",
        "Today, we will deal with the `tf.keras.layers.TextVectorization` layer which:\n",
        "* turns ***raw strings*** into an **encoded representation** \n",
        "* that representation can be read by an `Embedding` layer or `Dense` layer.\n",
        "\n",
        "That is, the `tf.keras.layers.TextVectorization` layer can be used in \n",
        "* **Text Preprocessing** and\n",
        "* **Text Vectorization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FKitY3ahAx7"
      },
      "source": [
        "# 2. IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b6TwpteGR7R"
      },
      "source": [
        "**IMPORTANT:** When I prepared this tutorial on 05 Oct 2021, the current version (2.6.0) of TF and Keras generate some **errors** in saving and uploading the **tf.keras.layers.TextVectorization layer**. \n",
        "\n",
        "However, the nightly version has no problem handling these operations.\n",
        "\n",
        "For more information about the bug, please see [here](https://github.com/keras-team/keras/issues/15443#issuecomment-938211510)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw5YiGjuH9pN"
      },
      "source": [
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"tf version:\",tf.__version__)\n",
        "\n",
        "print(\"keras version:\", keras.__version__)\n",
        "\n",
        "tf version: 2.6.0\n",
        "\n",
        "keras version: 2.6.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XAjNqIse9pJ"
      },
      "source": [
        "Therefore, below I first upload the TF nightly version. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5bmtFNee07h"
      },
      "source": [
        "```python\n",
        "tf version: 2.8.0-dev20211005\n",
        "keras version: 2.7.0\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPRv2g92Polb"
      },
      "source": [
        "pip install tf-nightly --quiet --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsDBX1yETA3v"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RqP1sQpRhf5",
        "outputId": "b7012e24-2764-4830-82b9-581f39d496c7"
      },
      "source": [
        "print(\"tf version:\",tf.__version__)\n",
        "print(\"keras version:\", keras.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf version: 2.8.0-dev20211203\n",
            "keras version: 2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bPDDB871W56",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a80e4cd-b581-4bde-ae9a-e27942c3601f"
      },
      "source": [
        "#@title Record Each Cell's Execution Time\n",
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 204 µs (started: 2021-12-03 14:12:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqcyE0LzKL-4"
      },
      "source": [
        "# 3. DOWNLOAD A KAGGLE DATASET INTO GOOGLE COLAB\n",
        "\n",
        "The [Multi Class Classification Dataset for Turkish](https://www.kaggle.com/savasy/multiclass-classification-data-for-turkish-tc32?select=ticaret-yorum.csv) is a **benchmark dataset for Turkish** **text classification** task. \n",
        "\n",
        "It contians 430K comments/reviews for a total 32 categories products or services.\n",
        "\n",
        "Each category roughly has 13K comments.\n",
        "\n",
        "A baseline algoritm, Naive Bayes, gets %84 F1 score.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[My blog post explaning how to download Kaggle Datasets is here.](https://medium.com/analytics-vidhya/how-to-fetch-kaggle-datasets-into-google-colab-ea682569851a)\n",
        "\n",
        "My video tutorial explaning how to download Kaggle Datasets is here: [Turkish](https://youtu.be/ls47CPFU1vE)/[English](https://youtu.be/_rlt4mzLDLc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pIhSabUHQPB",
        "outputId": "9db6aa07-ba03-4110-d365-6dc988c6877c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "time: 1.66 ms (started: 2021-12-03 14:12:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyAOsTtRIaSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0898e964-97d1-4253-ea4d-595c313894e0"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Colab Notebooks/input\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 850 µs (started: 2021-12-03 14:12:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGE3TaZtIsIl",
        "outputId": "a5a97b5e-d76b-487e-c495-17040bae3974"
      },
      "source": [
        "#changing the working directory\n",
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/input\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/input\n",
            "time: 3.1 ms (started: 2021-12-03 14:12:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4o__jkIwI-63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "487123fc-d480-4f9d-84e5-d7ff68b7c42f"
      },
      "source": [
        "#get the api command from kaggle dataset page\n",
        "#!kaggle datasets download -d savasy/multiclass-classification-data-for-turkish-tc32"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 570 µs (started: 2021-12-03 14:12:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cto_q1OUKQ9",
        "outputId": "93e6ecdd-dcdb-4638-aba5-77da4bcb37d1"
      },
      "source": [
        "# check the downloaded zip file\n",
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120001_PH1.csv\tgeneratedReviews.csv\t    kaggle.json        tr_stop_word.txt\n",
            "320d.csv\tgeneratedReviews_final.csv  model.png\t       vocabPickle\n",
            "corona.csv\tgeneratedReviews_plus.csv   ticaret-yorum.csv\n",
            "time: 152 ms (started: 2021-12-03 14:12:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViTkX2CbLTUe",
        "outputId": "9818ae07-152a-4c0f-83cf-aa75d00b0b15"
      },
      "source": [
        "# unzipping the zip files and deleting the zip files\n",
        "!unzip \\*.zip  && rm *.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open *.zip, *.zip.zip or *.zip.ZIP.\n",
            "\n",
            "No zipfiles found.\n",
            "time: 139 ms (started: 2021-12-03 14:12:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvogE3ztLndZ",
        "outputId": "4afed10d-e2bf-45ea-c113-052a099ea675"
      },
      "source": [
        "# check the downloaded csv file\n",
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120001_PH1.csv\tgeneratedReviews.csv\t    kaggle.json        tr_stop_word.txt\n",
            "320d.csv\tgeneratedReviews_final.csv  model.png\t       vocabPickle\n",
            "corona.csv\tgeneratedReviews_plus.csv   ticaret-yorum.csv\n",
            "time: 136 ms (started: 2021-12-03 14:12:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeJakYWhG9W1"
      },
      "source": [
        "# 4. LOAD STOP WORDS IN TURKISH\n",
        "\n",
        "As you might know \"**Stop words**\" are a set of commonly used words in a language. Examples of stop words in **English** are “a”, “the”, “is”, “are” and etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to **eliminate** words that are so commonly used that they carry **very little useful information**.\n",
        "\n",
        "I begin with uploading an existing  list of stop words in Turkish below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y0t0dhJG77-",
        "outputId": "5f062671-e285-4610-d921-ff559767b706"
      },
      "source": [
        "tr_stop_words = pd.read_csv('tr_stop_word.txt',header=None)\n",
        "for each in tr_stop_words.values[:5]:\n",
        "  print(each[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ama\n",
            "amma\n",
            "anca\n",
            "ancak\n",
            "bu\n",
            "time: 13.2 ms (started: 2021-12-03 14:12:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WQNplaQhmB3"
      },
      "source": [
        "# 5. LOAD THE DATASET\n",
        "After downloading the dataset from Kaggle website, we can upload it by using the Pandas library `read_csv()` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v0as4QesGQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2223dec-24d4-4a18-8de2-32097c5dcc84"
      },
      "source": [
        "data = pd.read_csv('ticaret-yorum.csv')\n",
        "pd.set_option('max_colwidth', 400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.34 s (started: 2021-12-03 14:12:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHr3M_p1ctKe"
      },
      "source": [
        "# **PART B: KNOW THE DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVGStvwEesYR"
      },
      "source": [
        "# 6. EXPLORE THE DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C0H8DoiguLw"
      },
      "source": [
        "Before getting into the details of how to use the `tf.keras.layers.TextVectorization` layer, let me introduce the dataset briefly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-eRYB-PlQsx"
      },
      "source": [
        "## Shuffle Data\n",
        "\n",
        "It is a really good and useful habit that, before doing anything else, as a first step in the preprocessing shuffle the data!\n",
        "\n",
        "Actually, I will shuffle the data at the last step of the pipeline.\n",
        "But it does not hurt shuffling it twice :))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAIVvhLjlvYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de46d81a-8911-42fd-fcb1-8a9ba6332fda"
      },
      "source": [
        "data= data.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 98.4 ms (started: 2021-12-03 14:12:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27UOqnurdk-B"
      },
      "source": [
        "## Summary Information about the dataset\n",
        "\n",
        "Get the initial information about the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2H1RqPks2tL",
        "outputId": "9eefb4a7-764d-4de0-ad88-3e4cfad48aa0"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 431306 entries, 71903 to 142963\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count   Dtype \n",
            "---  ------    --------------   ----- \n",
            " 0   category  431306 non-null  object\n",
            " 1   text      431306 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 9.9+ MB\n",
            "time: 107 ms (started: 2021-12-03 14:12:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynP1CD8oVtv_"
      },
      "source": [
        "We have a total of **431306** of rows and **2** columns: ***category*** & ***text***.\n",
        "\n",
        "According to `data.info()`, there is **no null values** in the dataset. If there are any null values in the dataset, we could drop these null values as follows:\n",
        "```python\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "df.isnull().sum()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31JouHmqVtCm"
      },
      "source": [
        "## Sample Reviews and their categories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "rzfkiUERVz5O",
        "outputId": "62b2df32-dff4-4db2-dc95-3d8839a66ef6"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71903</th>\n",
              "      <td>egitim</td>\n",
              "      <td>Benim Hocam Yayınevi 'den Yanlış Beyan,Benim Hocam Tarih soru bankasının kapağında tamamı çözümlü yazdığı için aldım ama 4 test kadar ilerleyince fark ettim ki tamamı çözümlü değil her konunun son testleri çözümsüz bence bu yapılan ‘beyanda sahtecilik’ hiç yakıştıramadım bu yayınevine başka bir kaynağını satın almayı düşünmüyorum.Devamını oku</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260078</th>\n",
              "      <td>kucuk-ev-aletleri</td>\n",
              "      <td>ECA Petek Ses Yapıyor,Geçen yıl yeni yaptırmış olduğumuz doğalgaz tesisatında ECA marka petekler kullanıldı. Peteklerden birinden sürekli tak tuk sesler geliyor. İlgili servis önce ses duymadığını sonra da sorunu ECA merkeze ilettiklerini bir sonuç alınamadığını söyledi. Yapıldığı günden beri bu şekilde arızalı bir ürünü...Devamını oku</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46503</th>\n",
              "      <td>bilgisayar</td>\n",
              "      <td>TP-Link W9970V3 Modem Isınma Ve Sararma!,\"TP-Link TD-W9970v3 modelini 1 yıldır kullanıyorum.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263875</th>\n",
              "      <td>kucuk-ev-aletleri</td>\n",
              "      <td>Philips Ütü Patladı Patlayacak!,\"Görselde ki ütüyü 5-6 ay önce, Beylikdüzü 5m Migros Philips bayiinden aldık. Aldığımızdan beri sürekli sorun çıkartıyor, aldığımız yere götürdük fakat bir problem olmadığını iddia ediyorlar.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109121</th>\n",
              "      <td>enerji</td>\n",
              "      <td>İgdaş Fatura İtirazı Sonucu!,\"Faturam 164 TL geldi ocak ayında bile bu kadar fatura gelmedi. Araştırılıp para iademi istiyorum. Corona'dan dolayı çalışamıyoruz bir de sürekli mesaj atıp atıp duruyorsunuz gecikme zammı alınacak diye.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 category                                                                                                                                                                                                                                                                                                                                                      text\n",
              "71903              egitim  Benim Hocam Yayınevi 'den Yanlış Beyan,Benim Hocam Tarih soru bankasının kapağında tamamı çözümlü yazdığı için aldım ama 4 test kadar ilerleyince fark ettim ki tamamı çözümlü değil her konunun son testleri çözümsüz bence bu yapılan ‘beyanda sahtecilik’ hiç yakıştıramadım bu yayınevine başka bir kaynağını satın almayı düşünmüyorum.Devamını oku\n",
              "260078  kucuk-ev-aletleri         ECA Petek Ses Yapıyor,Geçen yıl yeni yaptırmış olduğumuz doğalgaz tesisatında ECA marka petekler kullanıldı. Peteklerden birinden sürekli tak tuk sesler geliyor. İlgili servis önce ses duymadığını sonra da sorunu ECA merkeze ilettiklerini bir sonuç alınamadığını söyledi. Yapıldığı günden beri bu şekilde arızalı bir ürünü...Devamını oku\n",
              "46503          bilgisayar                                                                                                                                                                                                                                                              TP-Link W9970V3 Modem Isınma Ve Sararma!,\"TP-Link TD-W9970v3 modelini 1 yıldır kullanıyorum.\n",
              "263875  kucuk-ev-aletleri                                                                                                                           Philips Ütü Patladı Patlayacak!,\"Görselde ki ütüyü 5-6 ay önce, Beylikdüzü 5m Migros Philips bayiinden aldık. Aldığımızdan beri sürekli sorun çıkartıyor, aldığımız yere götürdük fakat bir problem olmadığını iddia ediyorlar.\n",
              "109121             enerji                                                                                                                  İgdaş Fatura İtirazı Sonucu!,\"Faturam 164 TL geldi ocak ayında bile bu kadar fatura gelmedi. Araştırılıp para iademi istiyorum. Corona'dan dolayı çalışamıyoruz bir de sürekli mesaj atıp atıp duruyorsunuz gecikme zammı alınacak diye."
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 19.1 ms (started: 2021-12-03 14:12:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqjl4QZHvAxK"
      },
      "source": [
        "# 7. CREATE A TENSORFLOW DATA PIPELINE FOR TEXT PREPROCESSING &  VECTORIZATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU26jUANffDY"
      },
      "source": [
        "So far, we just observe some properties of the **raw data**.\n",
        "Using these observations, we are ready to preprocess the `text` data for a classifier model.\n",
        "\n",
        "Below, we will begin to create a **TensorFlow data pipeline** which includes **Keras Text Vectorization layer** for preprocessing the data and preparing it for a classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfxrJ2-Agb7m"
      },
      "source": [
        "A pipeline for a text model mostly involves extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths.\n",
        "\n",
        "In this tutorial, I will use the TensorFlow \"**tf.data**\" API. If you are not familiar with TF data pipeline \"**tf.data**\" API, you can apply below resources:\n",
        "* Official TensorFlow blog: [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data) \n",
        "* The Murat Karakaya Akademi YouTube playlist in Turkish: [tf.data: TensorFlow Data Pipeline Anlamak ve Kullanmak](https://www.youtube.com/playlist?list=PLQflnv_s49v8l8dYU01150vcoAn4sWSAm)  \n",
        "* The Murat Karakaya Akademi YouTube playlist in English:[TensorFlow Data Pipeline: How to Design Code Use TensorFlow Data Pipelines with Python & Keras](https://www.youtube.com/playlist?list=PLQflnv_s49v_m6KLMsORgs9hVIvDCwDAb)\n",
        "* The Murat Karakaya Akademi Medium blog: [tf.data: Tensorflow Data Pipelines](https://medium.com/deep-learning-with-keras/tf-data-tensorflow-data-pipelines-71915155bdf2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVSPJfBxlmN8"
      },
      "source": [
        "## Convert Categories From Strings to Integer Ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhGRDmGZjhBt"
      },
      "source": [
        "Observe that the categories (topics/class)of the reviews are **strings**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytzcLGSHbKrw",
        "outputId": "ccab2d47-4054-40f9-e048-2573d81c5dd6"
      },
      "source": [
        "data[\"category\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71903                        egitim\n",
              "260078            kucuk-ev-aletleri\n",
              "46503                    bilgisayar\n",
              "263875            kucuk-ev-aletleri\n",
              "109121                       enerji\n",
              "                    ...            \n",
              "164858                        giyim\n",
              "419530                       ulasim\n",
              "247721    kisisel-bakim-ve-kozmetik\n",
              "163826                        giyim\n",
              "142963                       finans\n",
              "Name: category, Length: 431306, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 8.82 ms (started: 2021-12-03 14:12:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-2z9GHNiA8Y"
      },
      "source": [
        "We nned to create **integer** category **ids** from **string** category **names** by adding a new column to the dataframe \"**category_id**\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "7n8IFrNeiU7W",
        "outputId": "b2d485eb-728c-44c8-f02a-b381c1f4fbbb"
      },
      "source": [
        "data[\"category\"] = data[\"category\"].astype('category')\n",
        "data[\"category_id\"] = data[\"category\"].cat.codes\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>category_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71903</th>\n",
              "      <td>egitim</td>\n",
              "      <td>Benim Hocam Yayınevi 'den Yanlış Beyan,Benim Hocam Tarih soru bankasının kapağında tamamı çözümlü yazdığı için aldım ama 4 test kadar ilerleyince fark ettim ki tamamı çözümlü değil her konunun son testleri çözümsüz bence bu yapılan ‘beyanda sahtecilik’ hiç yakıştıramadım bu yayınevine başka bir kaynağını satın almayı düşünmüyorum.Devamını oku</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260078</th>\n",
              "      <td>kucuk-ev-aletleri</td>\n",
              "      <td>ECA Petek Ses Yapıyor,Geçen yıl yeni yaptırmış olduğumuz doğalgaz tesisatında ECA marka petekler kullanıldı. Peteklerden birinden sürekli tak tuk sesler geliyor. İlgili servis önce ses duymadığını sonra da sorunu ECA merkeze ilettiklerini bir sonuç alınamadığını söyledi. Yapıldığı günden beri bu şekilde arızalı bir ürünü...Devamını oku</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46503</th>\n",
              "      <td>bilgisayar</td>\n",
              "      <td>TP-Link W9970V3 Modem Isınma Ve Sararma!,\"TP-Link TD-W9970v3 modelini 1 yıldır kullanıyorum.</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263875</th>\n",
              "      <td>kucuk-ev-aletleri</td>\n",
              "      <td>Philips Ütü Patladı Patlayacak!,\"Görselde ki ütüyü 5-6 ay önce, Beylikdüzü 5m Migros Philips bayiinden aldık. Aldığımızdan beri sürekli sorun çıkartıyor, aldığımız yere götürdük fakat bir problem olmadığını iddia ediyorlar.</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109121</th>\n",
              "      <td>enerji</td>\n",
              "      <td>İgdaş Fatura İtirazı Sonucu!,\"Faturam 164 TL geldi ocak ayında bile bu kadar fatura gelmedi. Araştırılıp para iademi istiyorum. Corona'dan dolayı çalışamıyoruz bir de sürekli mesaj atıp atıp duruyorsunuz gecikme zammı alınacak diye.</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 category  ... category_id\n",
              "71903              egitim  ...           5\n",
              "260078  kucuk-ev-aletleri  ...          19\n",
              "46503          bilgisayar  ...           3\n",
              "263875  kucuk-ev-aletleri  ...          19\n",
              "109121             enerji  ...           8\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 73.5 ms (started: 2021-12-03 14:12:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4MqVsZlix51"
      },
      "source": [
        "Lastly, we can check the number of categories. Note that it should be **32**: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsaO4v8Zj34I",
        "outputId": "8cfc8107-ec5e-4de3-e864-84a811e2f24c"
      },
      "source": [
        "data['category']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71903                        egitim\n",
              "260078            kucuk-ev-aletleri\n",
              "46503                    bilgisayar\n",
              "263875            kucuk-ev-aletleri\n",
              "109121                       enerji\n",
              "                    ...            \n",
              "164858                        giyim\n",
              "419530                       ulasim\n",
              "247721    kisisel-bakim-ve-kozmetik\n",
              "163826                        giyim\n",
              "142963                       finans\n",
              "Name: category, Length: 431306, dtype: category\n",
              "Categories (32, object): ['alisveris', 'anne-bebek', 'beyaz-esya', 'bilgisayar', ..., 'spor',\n",
              "                          'temizlik', 'turizm', 'ulasim']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 9.33 ms (started: 2021-12-03 14:12:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHdF7xZguBJy"
      },
      "source": [
        "## Build a Dictionary for id to text category (topic) look-up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwfM2IVJuDJx",
        "outputId": "c3476ed1-ffeb-4052-c037-95a04ca74d0e"
      },
      "source": [
        "id_to_category = pd.Series(data.category.values,index=data.category_id).to_dict()\n",
        "id_to_category"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'alisveris',\n",
              " 1: 'anne-bebek',\n",
              " 2: 'beyaz-esya',\n",
              " 3: 'bilgisayar',\n",
              " 4: 'cep-telefon-kategori',\n",
              " 5: 'egitim',\n",
              " 6: 'elektronik',\n",
              " 7: 'emlak-ve-insaat',\n",
              " 8: 'enerji',\n",
              " 9: 'etkinlik-ve-organizasyon',\n",
              " 10: 'finans',\n",
              " 11: 'gida',\n",
              " 12: 'giyim',\n",
              " 13: 'hizmet-sektoru',\n",
              " 14: 'icecek',\n",
              " 15: 'internet',\n",
              " 16: 'kamu-hizmetleri',\n",
              " 17: 'kargo-nakliyat',\n",
              " 18: 'kisisel-bakim-ve-kozmetik',\n",
              " 19: 'kucuk-ev-aletleri',\n",
              " 20: 'medya',\n",
              " 21: 'mekan-ve-eglence',\n",
              " 22: 'mobilya-ev-tekstili',\n",
              " 23: 'mucevher-saat-gozluk',\n",
              " 24: 'mutfak-arac-gerec',\n",
              " 25: 'otomotiv',\n",
              " 26: 'saglik',\n",
              " 27: 'sigortacilik',\n",
              " 28: 'spor',\n",
              " 29: 'temizlik',\n",
              " 30: 'turizm',\n",
              " 31: 'ulasim'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 82 ms (started: 2021-12-03 14:12:22 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AYZ7g0u1tEnt",
        "outputId": "e736b93a-dd7f-42f7-8490-4ec7f5a11458"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/input'"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.12 ms (started: 2021-12-03 14:30:38 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnxVEIVctDHz",
        "outputId": "9276e4cf-a4d9-45dd-f70a-4865a3e06d02"
      },
      "source": [
        "import pickle\n",
        "pkl_file = open(\"id_to_category.pkl\", \"wb\")\n",
        "pickle.dump(id_to_category, pkl_file)\n",
        "pkl_file.close()\n",
        "\n",
        "pkl_file = open(\"id_to_category.pkl\", \"rb\")\n",
        "uploaded_id_to_category = pickle.load(pkl_file)\n",
        "print(uploaded_id_to_category)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{5: 'egitim', 19: 'kucuk-ev-aletleri', 3: 'bilgisayar', 8: 'enerji', 28: 'spor', 1: 'anne-bebek', 31: 'ulasim', 2: 'beyaz-esya', 21: 'mekan-ve-eglence', 0: 'alisveris', 17: 'kargo-nakliyat', 11: 'gida', 30: 'turizm', 9: 'etkinlik-ve-organizasyon', 25: 'otomotiv', 7: 'emlak-ve-insaat', 16: 'kamu-hizmetleri', 15: 'internet', 13: 'hizmet-sektoru', 27: 'sigortacilik', 20: 'medya', 29: 'temizlik', 12: 'giyim', 6: 'elektronik', 18: 'kisisel-bakim-ve-kozmetik', 24: 'mutfak-arac-gerec', 4: 'cep-telefon-kategori', 26: 'saglik', 14: 'icecek', 10: 'finans', 23: 'mucevher-saat-gozluk', 22: 'mobilya-ev-tekstili'}\n",
            "time: 8.92 ms (started: 2021-12-03 14:32:12 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aym0dhZz-byL"
      },
      "source": [
        "## Reduce the Size of the Dataset\n",
        "\n",
        "Since using a large dataset for **testing** your pipeline would take more time, you would prefer **take a portion** of the raw dataset as below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhQjlJ9CCbO0",
        "outputId": "51c8d098-6a04-44c1-bf85-607e2bc8931b"
      },
      "source": [
        "#limit the number of samples to be used in testing the pipeline\n",
        "#data_size= 1000 #instead of 431306 \n",
        "#data= data[:data_size]\n",
        "#data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.55 ms (started: 2021-10-08 14:40:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt9KpOpKnMk7"
      },
      "source": [
        "## Split the Raw Dataset into Train and Test Datasets\n",
        "\n",
        "To prevent **data leakage** during preprocessing the text data, we need to split the text int Train and Test data sets. \n",
        "\n",
        "**Data leakage** refers to a mistake make by the creator of a machine learning model in which they accidentally share information between the test and training data-sets. Typically, when splitting a data-set into testing and training sets, the goal is to ensure that no data is shared between the two. This is because the test set’s purpose is to simulate real-world, unseen data. However, when evaluating a model, we do have full access to both our train and test sets, so it is up to us to ensure that no data in the training set is present in the test set.\n",
        "\n",
        "In our case, since we want to classify reviews, we have **not to use** test reviews in **text vectorization**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-qdVfWagEu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dcb4e14-e2f7-450c-a812-1460fe9ceec6"
      },
      "source": [
        "# save features and targets from the 'data'\n",
        "features, targets = data['text'], data['category_id']\n",
        "\n",
        "train_features, test_features, train_targets, test_targets = train_test_split(\n",
        "        features, targets,\n",
        "        train_size=0.8,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        shuffle = True,\n",
        "        stratify=targets\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 286 ms (started: 2021-10-08 14:40:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKuPtn1VoxnX"
      },
      "source": [
        "# Build the Train & Test TensorFlow Datasets\n",
        "\n",
        "First, we create **TensorFlow Datasets** from the raw Train Dataframe for further processing.\n",
        "\n",
        "Note that:\n",
        "1. **X**: input (text/reviews)\n",
        "2. **y**: target value (categories/topics/class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swV1vlGKocG_"
      },
      "source": [
        "**Observe that** we have **reviews in text** as input and **categories (topics) in integer** as target values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E5ba9Gjv0lb",
        "outputId": "7df332f2-3f68-43a7-cf8d-88f45b68c1c3"
      },
      "source": [
        "train_features.values[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['İçim Kaşar Peynir İçinden Yeşil Madde,Kaşar peynirin içinden maydanoza benzer yeşil bir madde çıktı biz bunu fark etmeden yiyebiliriz de lütfen yetkililerden bir açıklama bekliyorum bu gıda maddesinin içinde ne gibi bir madde olabilir. Bize nasıl ortamlarda ürettiğiniz ürünleri yediriyorsunuz kesinlikle küf değil fotoğrafını da ekliyoru...Devamını oku',\n",
              "       'Philips TV İnternet Bağlantı Sorunu!,\"Philips 32PFS5803/62 model Smart televizyonumu Vatan markete henüz 1 ay oldu alalı 1 ay her yere bağlanan TV internete bağlı olmasına rağmen YouTube.com, Smart TV, uygulama galerisi vb... Hiçbir uygulamayı açmıyor. Girmeye çalıştığım zaman \"\"bu TV\\'yi internete bağlayın\"\" sayfası açılıyor ve bağlamaya ...Devamını oku\"',\n",
              "       'Anadolu Hastanesi (Çanakkale) Muayene Süresi Kısalığı,20 aylık çocuğum var devamlı çocuk Dr. y. A muayene oluyorum ama artık aynı sorunla karşılaşmaktan bıktım. Alel acele 5 dakikada muayene yapıor hastanın çıkmasını beklemeden yeni hasta alıyor ve onun yanında çocuk giydiriliyor belki özel konuşacaklarmış ya da özel durumumuz var düşünen yok. Paramızl...Devamını oku',\n",
              "       'Digiturk Engelsiz Kampanyası Zulmü!,1014917147 numaralı aboneliğimle ilgili. Digiturk pazarlama stratejisi ile insanları örtülü olarak resmen yanıltıyor. Engelli indiriminden taahhütsüz olarak üye oldum. Sonra iptal etmek istedim 70 TL cayma bedeli talep ettiler. Taahhütsüz dedim ilk başta kurum yapıldı 1 yıl içinde iptal edilirse kur...Devamını oku',\n",
              "       'Rowenta Elektrik Süpürge İyi Çekmiyor!,RO3723TA-JSO-3617 ürün kodlu Rowenta marka elektrik süpürgemi 18 aydır kullanmama rağmen iyi çekmediği için Çanakkale servisine götürdüm ve garantisi bile henüz dolmayan süpürge için filtre temizliği yapılacağından 130 TL ücret istenmektedir. Daha yeni süpürge hem çekmiyor hem de filtre temizliği iç...Devamını oku'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7.27 ms (started: 2021-10-08 14:40:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJTyncnJwOe9",
        "outputId": "1c1574f1-4169-4860-ae94-a0f3bf2479a5"
      },
      "source": [
        "train_targets.values[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11,  6, 26, 20, 19], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.68 ms (started: 2021-10-08 14:40:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMwYNTks2WZS"
      },
      "source": [
        "## Prepare TensorFlow Datasets\n",
        "\n",
        "We convert the data stored in Pandas Data Frame into  a data stored in TensorFlow Data Set as below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyNh1kCskjVY",
        "outputId": "8f35f280-126d-49ad-eb84-8c922feb9566"
      },
      "source": [
        "# train X & y\n",
        "train_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(train_features.values, tf.string)\n",
        ") \n",
        "train_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(train_targets.values, tf.int64),\n",
        "\n",
        ") \n",
        "# test X & y\n",
        "test_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(test_features.values, tf.string)\n",
        ") \n",
        "test_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "            tf.cast(test_targets.values, tf.int64),\n",
        "\n",
        ") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.81 s (started: 2021-10-08 14:40:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9oAjlqipore"
      },
      "source": [
        "## Decide the dictionary size and the review size\n",
        "\n",
        "For preprocessing the text, we need to decide the **dictionary (vocabulary) size** and the **review (text) length**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2frnPx9C4wE6"
      },
      "source": [
        "vocab_size = 20000  # Only consider the top 20K words\n",
        "max_len = 50  # Maximum review (text) size in words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye1tSOhYdZeX"
      },
      "source": [
        "# **PART C: USE KERAS TEXT VECTORIZATION LAYER**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiGIXHdKnpBX"
      },
      "source": [
        "# 8. PREPROCESS THE TEXT WITH THE KERAS `TEXTVECTORIZATION` LAYER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pK1RrG8rROz"
      },
      "source": [
        "\n",
        "\n",
        "## 8.1. Define your own `custom_standardization` function\n",
        "First, I define a function which will preprocess the given text.\n",
        "The `custom_standardization` function will convert the given string to a standart form by transforming the input applying several updates:\n",
        "* convert all characters to lowercase\n",
        "* remove special symbols, extra spaces, html tags, digits, and puctuations\n",
        "* remove stop wrods\n",
        "* replace the special Turkish letters with the corresponding English letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLdLF0qQBH1"
      },
      "source": [
        "@tf.keras.utils.register_keras_serializable()\n",
        "def custom_standardization(input_string):\n",
        "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
        "    no_uppercased = tf.strings.lower(input_string, encoding='utf-8')\n",
        "    no_stars = tf.strings.regex_replace(no_uppercased, \"\\*\", \" \")\n",
        "    no_repeats = tf.strings.regex_replace(no_stars, \"devamını oku\", \"\")    \n",
        "    no_html = tf.strings.regex_replace(no_repeats, \"<br />\", \"\")\n",
        "    no_digits = tf.strings.regex_replace(no_html, \"\\w*\\d\\w*\",\"\")\n",
        "    no_punctuations = tf.strings.regex_replace(no_digits, f\"([{string.punctuation}])\", r\" \")\n",
        "    #remove stop words\n",
        "    no_stop_words = ' '+no_punctuations+ ' '\n",
        "    for each in tr_stop_words.values:\n",
        "      no_stop_words = tf.strings.regex_replace(no_stop_words, ' '+each[0]+' ' , r\" \")\n",
        "    no_extra_space = tf.strings.regex_replace(no_stop_words, \" +\",\" \")\n",
        "    #remove Turkish chars\n",
        "    no_I = tf.strings.regex_replace(no_extra_space, \"ı\",\"i\")\n",
        "    no_O = tf.strings.regex_replace(no_I, \"ö\",\"o\")\n",
        "    no_C = tf.strings.regex_replace(no_O, \"ç\",\"c\")\n",
        "    no_S = tf.strings.regex_replace(no_C, \"ş\",\"s\")\n",
        "    no_G = tf.strings.regex_replace(no_S, \"ğ\",\"g\")\n",
        "    no_U = tf.strings.regex_replace(no_G, \"ü\",\"u\")\n",
        "\n",
        "    return no_U"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2L0siQkzHBI"
      },
      "source": [
        "Quickly verify that `custom_standardization` works: try it on a sample Turkish input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M2IQsBBuy-a",
        "outputId": "d6137c52-0852-4b7a-ba73-325061937498"
      },
      "source": [
        "input_string = \"Bu Issız Öğlenleyin de;  şunu ***1 Pijamalı Hasta***, ve  Ancak İşte Yağız Şoföre Çabucak Güvendi...Devamını oku\"\n",
        "print(\"input:  \", input_string)\n",
        "output_string= custom_standardization(input_string)\n",
        "print(\"output: \", output_string.numpy().decode(\"utf-8\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:   Bu Issız Öğlenleyin de;  şunu ***1 Pijamalı Hasta***, ve  Ancak İşte Yağız Şoföre Çabucak Güvendi...Devamını oku\n",
            "output:   issiz oglenleyin pijamali hasta i̇ste yagiz sofore cabucak guvendi \n",
            "time: 58.8 ms (started: 2021-10-08 14:40:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkigEVBvjqmk"
      },
      "source": [
        "## 8.2. Configure the Keras `TextVectorization` layer\n",
        "\n",
        "To preprocess the text, I will use the Keras `TextVectorization` layer. \n",
        "\n",
        "```python\n",
        "tf.keras.layers.TextVectorization(\n",
        "    max_tokens=None,\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    split=\"whitespace\",\n",
        "    ngrams=None,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=None,\n",
        "    pad_to_max_tokens=False,\n",
        "    vocabulary=None,\n",
        "    **kwargs\n",
        ")\n",
        "```\n",
        "\n",
        "The Keras `TextVectorization` layer processes each example in the dataset as follows:\n",
        "\n",
        "1. Standardize each example (usually lowercasing + punctuation stripping)\n",
        "\n",
        "2. Split each example into substrings (usually words)\n",
        "\n",
        "3. Recombine substrings into tokens (usually ngrams)\n",
        "\n",
        "4. Index tokens (associate a unique int value with each token)\n",
        "\n",
        "5. Transform each example using this index, either into a vector of ints or a dense float vector.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI50-CQ27X-G"
      },
      "source": [
        "Let's build our `TextVectorization` layer by providing:\n",
        "\n",
        "1. The `custom_standardization()` function for the `standardize` method (callable).\n",
        "2. The `vocab_size` as the `max_tokens` number: The `max_tokens` is the maximum size of the vocabulary that will be created from the dataset. If `None`, there is no cap on the size of the vocabulary. Note that this vocabulary contains 1 **OOV (Out Of Vocabulary)** token, so the effective number of tokens is (max_tokens - 1 - (1 if output_mode == \"int\" else 0)).\n",
        "3. The `int` keyword as the `output_mode`: Optional specification for the **output** of the layer. Values can be \n",
        "* \"**int**\", \n",
        "* \"**multi_hot**\", \n",
        "* \"**count**\" or \n",
        "* \"**tf_idf**\", \n",
        "\n",
        "Configuring the layer as follows: \n",
        "* \"**int**\": Outputs integer indices, one integer index per split string token. When output_mode == \"int\", 0 is reserved for masked locations; this reduces the vocab size to max_tokens - 2 instead of max_tokens - 1.\n",
        "\n",
        "* \"**multi_hot**\": Outputs a single int array per batch, of either vocab_size or max_tokens size, containing 1s in all elements where the token mapped to that index exists at least once in the batch item. \n",
        "\n",
        "* \"**count**\": Like \"multi_hot\", but the int array contains a count of the number of times the token at that index appeared in the batch item. \n",
        "\n",
        "* \"**tf_idf**\": Like \"multi_hot\", but the TF-IDF algorithm is applied to find the value in each token slot. \n",
        "\n",
        "For \"**int**\" output, any shape of input and output is supported. \n",
        "\n",
        "For **all other output modes**, currently only **rank 1 inputs** (and rank 2 outputs after splitting) are supported. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-8JyLj68M5j"
      },
      "source": [
        "4. output_sequence_length=max_len"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbdKK8Uc4ko-"
      },
      "source": [
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size+2,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_len,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsc8CNBeldYR"
      },
      "source": [
        "## 8.3. Adapt the Keras `TextVectorization` layer with the **training** data set, (not test data set!) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNSv_ZM9lTKe"
      },
      "source": [
        "`TextVectorization` preprocessing layer has an internal state that can be computed based on a sample of the training data. That is, `TextVectorization` holds a **mapping** between **string** tokens and integer **indices**.\n",
        "\n",
        "Thus, we will ***adopt*** `TextVectorization` preprocessing layer **ONLY** to the **training** data.\n",
        "\n",
        "\n",
        "**Please note that:** To prevent and data leak, we **DO NOT** adopt `TextVectorization` preprocessing layer to the **whole** (***train & test***) data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8nd1qGXwy-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce7f5c10-45fb-46df-cc79-81a165807293"
      },
      "source": [
        "vectorize_layer.adapt(train_features)\n",
        "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2min 22s (started: 2021-10-08 14:40:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOyv5s64AD1J"
      },
      "source": [
        "Let's see some example conversions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQpBexwy5Fsy",
        "outputId": "fa691fd4-0982-4455-b557-1e69c11bd2e7"
      },
      "source": [
        "print(\"vocab has the \", len(vocab),\" entries\")\n",
        "print(\"vocab has the following first 10 entries\")\n",
        "for word in range(10):\n",
        "  print(word, \" represents the word: \", vocab[word])\n",
        "\n",
        "for X in train_features[:2]:\n",
        "  print(\" Given raw data: \" )\n",
        "  print(X)\n",
        "  tokenized = vectorize_layer(tf.expand_dims(X, -1))\n",
        "  print(\" Tokenized and Transformed to a vector of integers: \" )\n",
        "  print (tokenized)\n",
        "  print(\" Text after Tokenized and Transformed: \")\n",
        "  transformed = \"\"\n",
        "  for each in tf.squeeze(tokenized):\n",
        "    transformed= transformed+ \" \"+ vocab[each]\n",
        "  print(transformed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab has the  20002  entries\n",
            "vocab has the following first 10 entries\n",
            "0  represents the word:  \n",
            "1  represents the word:  [UNK]\n",
            "2  represents the word:  ne\n",
            "3  represents the word:  tl\n",
            "4  represents the word:  gun\n",
            "5  represents the word:  urun\n",
            "6  represents the word:  aldim\n",
            "7  represents the word:  siparis\n",
            "8  represents the word:  musteri\n",
            "9  represents the word:  tarihinde\n",
            " Given raw data: \n",
            "İçim Kaşar Peynir İçinden Yeşil Madde,Kaşar peynirin içinden maydanoza benzer yeşil bir madde çıktı biz bunu fark etmeden yiyebiliriz de lütfen yetkililerden bir açıklama bekliyorum bu gıda maddesinin içinde ne gibi bir madde olabilir. Bize nasıl ortamlarda ürettiğiniz ürünleri yediriyorsunuz kesinlikle küf değil fotoğrafını da ekliyoru...Devamını oku\n",
            " Tokenized and Transformed to a vector of integers: \n",
            "tf.Tensor(\n",
            "[[ 3451  3133  1770  1566  1605  1709  3133  6372   640     1  2025  1605\n",
            "   1709    64   209  2335     1  4024   853   184  1037     1    72     2\n",
            "   1709   623   177     1 18408   367     1   282  2582  3586     1     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0]], shape=(1, 50), dtype=int64)\n",
            " Text after Tokenized and Transformed: \n",
            " i̇cim kasar peynir i̇cinden yesil madde kasar peynirin icinden [UNK] benzer yesil madde cikti fark etmeden [UNK] yetkililerden aciklama bekliyorum gida [UNK] icinde ne madde olabilir bize [UNK] urettiginiz urunleri [UNK] kesinlikle kuf fotografini [UNK]               \n",
            " Given raw data: \n",
            "Philips TV İnternet Bağlantı Sorunu!,\"Philips 32PFS5803/62 model Smart televizyonumu Vatan markete henüz 1 ay oldu alalı 1 ay her yere bağlanan TV internete bağlı olmasına rağmen YouTube.com, Smart TV, uygulama galerisi vb... Hiçbir uygulamayı açmıyor. Girmeye çalıştığım zaman \"\"bu TV'yi internete bağlayın\"\" sayfası açılıyor ve bağlamaya ...Devamını oku\"\n",
            " Tokenized and Transformed to a vector of integers: \n",
            "tf.Tensor(\n",
            "[[  226    44   354  1078    17   226   215   206  9049  1079  2556   460\n",
            "     11   574    11    19   294 13253    44  2481  1384   124   648   141\n",
            "    206    44   672     1  2262    22  2862   890  5564  2058    67    44\n",
            "    469  2481     1  4955  1862 15099     0     0     0     0     0     0\n",
            "      0     0]], shape=(1, 50), dtype=int64)\n",
            " Text after Tokenized and Transformed: \n",
            " philips tv i̇nternet baglanti sorunu philips model smart televizyonumu vatan markete henuz ay alali ay her yere baglanan tv internete bagli olmasina youtube com smart tv uygulama [UNK] vb hicbir uygulamayi acmiyor girmeye calistigim zaman tv yi internete [UNK] sayfasi aciliyor baglamaya        \n",
            "time: 157 ms (started: 2021-10-08 14:42:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2Jm91uo8GL9",
        "outputId": "7b565ad7-df68-4697-e79d-672a6da0bfdb"
      },
      "source": [
        "vocab[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'ne', 'tl', 'gun']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 4.75 ms (started: 2021-10-08 14:42:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwLTEYa7a2aD"
      },
      "source": [
        "## 8.4. Save & Upload TextVectorization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9geoLkTlzJD"
      },
      "source": [
        "Due to the facts that adapting the Keras `TextVectorization` layer on a large text dataset takes considerable amount of time and porting the adapted layer to a different deployment environment is a high possibility, it is good to know how to save and load it.\n",
        "\n",
        "How to save a Keras `TextVectorization` layer? \n",
        "\n",
        "[There are currently 2 ways of doing it](https://stackoverflow.com/questions/65103526/how-to-save-textvectorization-to-disk-in-tensorflow):\n",
        "* save the Keras `TextVectorization` layer in a Keras Model\n",
        "* save the Keras `TextVectorization` layer as a pickle file.\n",
        "\n",
        "In this tutorial, I will use the first approach as it is native to the TF/Keras environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhXnpHRDntyl"
      },
      "source": [
        "### 8.4.1. Ensure that you are on the correct directory path :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHwRco9l89C9",
        "outputId": "814d3380-a937-4616-fe72-5f4af2a3ab9a"
      },
      "source": [
        "%cd ../models/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/models\n",
            "\u001b[0m\u001b[01;34mcheckpoint\u001b[0m/                    \u001b[01;34mMultiClassTextClassificationExported\u001b[0m/\n",
            "\u001b[01;34mend_to_end_model\u001b[0m/              \u001b[01;34mMultitopicTextGenerator\u001b[0m/\n",
            "\u001b[01;34mMultiClassTextClassification\u001b[0m/  \u001b[01;34mvectorize_layer_model\u001b[0m/\n",
            "time: 366 ms (started: 2021-10-08 14:42:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcsxmgbFn33v"
      },
      "source": [
        "### 8.4.2. Create a temporary Keras `model` by adding the adapted Keras `TextVectorization` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRvfjtqQa8Wa",
        "outputId": "d284c630-9adb-40c4-83ae-a5e158cc893a"
      },
      "source": [
        "# Create model.\n",
        "vectorize_layer_model = tf.keras.models.Sequential()\n",
        "vectorize_layer_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "vectorize_layer_model.add(vectorize_layer)\n",
        "vectorize_layer_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 50)               0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 256 ms (started: 2021-10-08 14:42:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9MlEks0oG9S"
      },
      "source": [
        "## 8.4.3. Save the temporary model including the adapted Keras `TextVectorization` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA0p_Nz6kAyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72880634-8868-47f9-8c4b-2f4a42770c82"
      },
      "source": [
        "filepath = \"vectorize_layer_model\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 721 µs (started: 2021-10-08 14:42:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5CCEtKHkEEU",
        "outputId": "201a6470-2f92-49c9-f26f-fbd7b140db90"
      },
      "source": [
        "vectorize_layer_model.save(filepath, save_format=\"tf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: vectorize_layer_model/assets\n",
            "time: 4.86 s (started: 2021-10-08 14:42:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYEdEVkY6K27",
        "outputId": "13635b39-f36a-40ce-e0fb-8a91bbfedc31"
      },
      "source": [
        "%ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoint\u001b[0m/                    \u001b[01;34mMultiClassTextClassificationExported\u001b[0m/\n",
            "\u001b[01;34mend_to_end_model\u001b[0m/              \u001b[01;34mMultitopicTextGenerator\u001b[0m/\n",
            "\u001b[01;34mMultiClassTextClassification\u001b[0m/  \u001b[01;34mvectorize_layer_model\u001b[0m/\n",
            "time: 153 ms (started: 2021-10-08 14:42:46 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMRIAizqoQt5"
      },
      "source": [
        "### 8.4.4. Load the `vectorize_layer_model` back to chek if saving is succesfull"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-BtuFg-5-yL",
        "outputId": "4cd7a68d-8980-4348-d145-5d8046460991"
      },
      "source": [
        "loaded_vectorize_layer_model = tf.keras.models.load_model(filepath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "time: 1.93 s (started: 2021-10-08 14:42:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06p5ZMh_ofxn"
      },
      "source": [
        "### 8.4.5 Retrieve the **loaded** Keras `TextVectorization` layer\n",
        "\n",
        "Here, you have 2 options:\n",
        "* use the `loaded_model.predicted()` method to use the Keras `TextVectorization` layer, or\n",
        "* get the Keras `TextVectorization` layer out of the `loaded_model` as below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMp7Hu7ipNSJ",
        "outputId": "62fa8c99-795b-418f-bf4e-c2cd237987f4"
      },
      "source": [
        "loaded_vectorize_layer = loaded_vectorize_layer_model.layers[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.97 ms (started: 2021-10-08 14:42:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfsI9cVipSUD"
      },
      "source": [
        "### 8.4.6. Compare the original and loaded `TextVectorization` layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk2CVBAjcsEx",
        "outputId": "a2e34492-b96e-4cbd-bc6f-37b8c474ef91"
      },
      "source": [
        "loaded_vocab=loaded_vectorize_layer.get_vocabulary()\n",
        "print(\"original vocab has the \", len(vocab),\" entries\")\n",
        "print(\"loaded vocab has the   \", len(loaded_vocab),\" entries\")\n",
        "print(\"loaded vocab has the following first 10 entries\")\n",
        "for word in range(10):\n",
        "  print(word, \" represents the word: \")\n",
        "  print(vocab[word], \" in original vocab\")\n",
        "  print(loaded_vocab[word], \" in loaded vocab\")\n",
        "for X in train_features[:1]:\n",
        "  print(\" Given raw data: \" )\n",
        "  print(X)\n",
        "\n",
        "  tokenized = vectorize_layer(tf.expand_dims(X, -1))\n",
        "  print(\" Tokenized and Transformed to a vector of integers by the original vectorize layer:\" )\n",
        "  print (tokenized)\n",
        "\n",
        "  tokenized = loaded_vectorize_layer(tf.expand_dims(X, -1))\n",
        "  print(\" Tokenized and Transformed to a vector of integers by the loaded vectorize layer:\" )\n",
        "  print (tokenized)\n",
        "  \n",
        "  tokenized = loaded_vectorize_layer_model.predict(tf.expand_dims(X, -1))\n",
        "  print(\" Tokenized and Transformed to a vector of integers by the loaded_vectorize_layer_model:\" )\n",
        "  print (tokenized)\n",
        "\n",
        "  print(\" Text after Tokenized and Transformed by the original vectorize layer:: \")\n",
        "  transformed = \"\"\n",
        "  for each in tf.squeeze(tokenized):\n",
        "    transformed= transformed+ \" \"+ vocab[each]\n",
        "  print(transformed)\n",
        "\n",
        "  print(\" Text after Tokenized and Transformed by the loaded vectorize layer:\")\n",
        "  transformed = \"\"\n",
        "  for each in tf.squeeze(tokenized):\n",
        "    transformed= transformed+ \" \"+ loaded_vocab[each]\n",
        "  print(transformed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original vocab has the  20002  entries\n",
            "loaded vocab has the    20002  entries\n",
            "loaded vocab has the following first 10 entries\n",
            "0  represents the word: \n",
            "  in original vocab\n",
            "  in loaded vocab\n",
            "1  represents the word: \n",
            "[UNK]  in original vocab\n",
            "[UNK]  in loaded vocab\n",
            "2  represents the word: \n",
            "ne  in original vocab\n",
            "ne  in loaded vocab\n",
            "3  represents the word: \n",
            "tl  in original vocab\n",
            "tl  in loaded vocab\n",
            "4  represents the word: \n",
            "gun  in original vocab\n",
            "gun  in loaded vocab\n",
            "5  represents the word: \n",
            "urun  in original vocab\n",
            "urun  in loaded vocab\n",
            "6  represents the word: \n",
            "aldim  in original vocab\n",
            "aldim  in loaded vocab\n",
            "7  represents the word: \n",
            "siparis  in original vocab\n",
            "siparis  in loaded vocab\n",
            "8  represents the word: \n",
            "musteri  in original vocab\n",
            "musteri  in loaded vocab\n",
            "9  represents the word: \n",
            "tarihinde  in original vocab\n",
            "tarihinde  in loaded vocab\n",
            " Given raw data: \n",
            "İçim Kaşar Peynir İçinden Yeşil Madde,Kaşar peynirin içinden maydanoza benzer yeşil bir madde çıktı biz bunu fark etmeden yiyebiliriz de lütfen yetkililerden bir açıklama bekliyorum bu gıda maddesinin içinde ne gibi bir madde olabilir. Bize nasıl ortamlarda ürettiğiniz ürünleri yediriyorsunuz kesinlikle küf değil fotoğrafını da ekliyoru...Devamını oku\n",
            " Tokenized and Transformed to a vector of integers by the original vectorize layer:\n",
            "tf.Tensor(\n",
            "[[ 3451  3133  1770  1566  1605  1709  3133  6372   640     1  2025  1605\n",
            "   1709    64   209  2335     1  4024   853   184  1037     1    72     2\n",
            "   1709   623   177     1 18408   367     1   282  2582  3586     1     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0]], shape=(1, 50), dtype=int64)\n",
            " Tokenized and Transformed to a vector of integers by the loaded vectorize layer:\n",
            "tf.Tensor(\n",
            "[[ 3451  3133  1770  1566  1605  1709  3133  6372   640     1  2025  1605\n",
            "   1709    64   209  2335     1  4024   853   184  1037     1    72     2\n",
            "   1709   623   177     1 18408   367     1   282  2582  3586     1     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0]], shape=(1, 50), dtype=int64)\n",
            " Tokenized and Transformed to a vector of integers by the loaded_vectorize_layer_model:\n",
            "[[ 3451  3133  1770  1566  1605  1709  3133  6372   640     1  2025  1605\n",
            "   1709    64   209  2335     1  4024   853   184  1037     1    72     2\n",
            "   1709   623   177     1 18408   367     1   282  2582  3586     1     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0]]\n",
            " Text after Tokenized and Transformed by the original vectorize layer:: \n",
            " i̇cim kasar peynir i̇cinden yesil madde kasar peynirin icinden [UNK] benzer yesil madde cikti fark etmeden [UNK] yetkililerden aciklama bekliyorum gida [UNK] icinde ne madde olabilir bize [UNK] urettiginiz urunleri [UNK] kesinlikle kuf fotografini [UNK]               \n",
            " Text after Tokenized and Transformed by the loaded vectorize layer:\n",
            " i̇cim kasar peynir i̇cinden yesil madde kasar peynirin icinden [UNK] benzer yesil madde cikti fark etmeden [UNK] yetkililerden aciklama bekliyorum gida [UNK] icinde ne madde olabilir bize [UNK] urettiginiz urunleri [UNK] kesinlikle kuf fotografini [UNK]               \n",
            "time: 787 ms (started: 2021-10-08 14:42:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PopPGDc4p0fZ"
      },
      "source": [
        "As you see above, we succesfully saved and loaded the *adapted* Keras `TextVectorization` layer!\n",
        "\n",
        "We can continue to the TensorFlow datapipeline with the **adapted** Keras `TextVectorization` layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IuA1NfTflVVh",
        "outputId": "770ece6b-3945-4fc7-d7e2-58342f52dd5d"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/models'"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 11.9 ms (started: 2021-10-08 14:42:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp0qIlQf0yym"
      },
      "source": [
        "# 9. APPLY KERAS `TEXTVECTORIZATION` TO TRAIN & TEST DATA SETS \n",
        "\n",
        "We can define a function to apply the Keras `TextVectorization` on a given string as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm1KFgd61PQs",
        "outputId": "513bdc60-7da3-4f81-ad05-c186230b34e5"
      },
      "source": [
        "def convert_text_input(sample):\n",
        "    text = sample\n",
        "    text = tf.expand_dims(text, -1)  \n",
        "    #return tf.squeeze(vectorize_layer(text))\n",
        "    return tf.squeeze(loaded_vectorize_layer(text)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.48 ms (started: 2021-10-08 14:42:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0t4sIVH12qk"
      },
      "source": [
        "We use the TensorFlow `tf.data` API (TF Data Pipeline) `map()` funtion to apply `convert_text_input()` on every sample in the `text` column (reviews) of the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESBeHt831vwc",
        "outputId": "232bfb93-a951-4f64-904e-837cb242a52e"
      },
      "source": [
        "# Train X\n",
        "train_text_ds = train_text_ds_raw.map(convert_text_input, \n",
        "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "# Test X\n",
        "test_text_ds = test_text_ds_raw.map(convert_text_input, \n",
        "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 696 ms (started: 2021-10-08 14:42:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6q3pkSZ2c2E"
      },
      "source": [
        "Let's see the converted/encoded texts (reviews)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVld41qV2kGZ",
        "outputId": "93dab5b2-6d6f-4c18-9f0f-f63b2dce8934"
      },
      "source": [
        "for each in train_text_ds.take(3):\n",
        "  print(each)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[ 3451  3133  1770  1566  1605  1709  3133  6372   640     1  2025  1605\n",
            "  1709    64   209  2335     1  4024   853   184  1037     1    72     2\n",
            "  1709   623   177     1 18408   367     1   282  2582  3586     1     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0], shape=(50,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[  226    44   354  1078    17   226   215   206  9049  1079  2556   460\n",
            "    11   574    11    19   294 13253    44  2481  1384   124   648   141\n",
            "   206    44   672     1  2262    22  2862   890  5564  2058    67    44\n",
            "   469  2481     1  4955  1862 15099     0     0     0     0     0     0\n",
            "     0     0], shape=(50,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[  465   171  3144   673   378     1   192  1280    10  1273   414  1023\n",
            "   695    74   673  3805   102    25  1777     1  1706     1  6537  2406\n",
            "   673     1  8569  9825  9478    79  1001   788   975   414     1   348\n",
            "     1    28   348 13025    10 11558     1     0     0     0     0     0\n",
            "     0     0], shape=(50,), dtype=int64)\n",
            "time: 154 ms (started: 2021-10-08 14:42:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHvbhCZk2rsk"
      },
      "source": [
        "10. GENERATE THE TRAIN SET BY COMBINING X & Y:\n",
        "* **X**: the preprocessed & encoded reviews \n",
        "* **y**: encoded categories) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOYpCBmV2xTk",
        "outputId": "03bce59a-0972-46c2-fe4a-5d14a0981280"
      },
      "source": [
        "train_ds = tf.data.Dataset.zip(\n",
        "    (\n",
        "            train_text_ds,\n",
        "            train_cat_ds_raw\n",
        "     )\n",
        ") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.9 ms (started: 2021-10-08 14:42:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS6Nje1IJi_D"
      },
      "source": [
        "Similarly, let's bundle test data sets as a single data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaQLHIYOJrng",
        "outputId": "f82e2d16-b1a7-4f91-f39d-e1d7c85f0b33"
      },
      "source": [
        "test_ds = tf.data.Dataset.zip(\n",
        "    (\n",
        "            test_text_ds,\n",
        "            test_cat_ds_raw\n",
        "     )\n",
        ") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.7 ms (started: 2021-10-08 14:42:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opoknhKL27Lr"
      },
      "source": [
        "We can see the result of the **Text Vectorization** in the **Data Pipelining** as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdqgVuiw3EOM",
        "outputId": "73d2af9f-1b2d-4b22-bd65-760eea2fda66"
      },
      "source": [
        "for X,y in train_ds.take(1):\n",
        "  print(\"input (review) X.shape: \", X.shape)\n",
        "  print(\"output (category) y.shape: \", y.shape)\n",
        "  print(\"input (review) X: \", X)\n",
        "  print(\"output (category) y: \",y)\n",
        "  input = \" \".join([vocab[_] for _ in np.squeeze(X)])\n",
        "  output = id_to_category[y.numpy()]\n",
        "  print(\"X: input (review) in text: \" , input)\n",
        "  print(\"y: output (category) in text: \" , output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input (review) X.shape:  (50,)\n",
            "output (category) y.shape:  ()\n",
            "input (review) X:  tf.Tensor(\n",
            "[ 3451  3133  1770  1566  1605  1709  3133  6372   640     1  2025  1605\n",
            "  1709    64   209  2335     1  4024   853   184  1037     1    72     2\n",
            "  1709   623   177     1 18408   367     1   282  2582  3586     1     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0], shape=(50,), dtype=int64)\n",
            "output (category) y:  tf.Tensor(11, shape=(), dtype=int64)\n",
            "X: input (review) in text:  i̇cim kasar peynir i̇cinden yesil madde kasar peynirin icinden [UNK] benzer yesil madde cikti fark etmeden [UNK] yetkililerden aciklama bekliyorum gida [UNK] icinde ne madde olabilir bize [UNK] urettiginiz urunleri [UNK] kesinlikle kuf fotografini [UNK]               \n",
            "y: output (category) in text:  gida\n",
            "time: 167 ms (started: 2021-10-08 14:42:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwBWJ_vQ3Q9d"
      },
      "source": [
        "# 11. FINALIZE TENSORFLOW DATA PIPELINE\n",
        "Finalize TensorFlow Data Pipeline by setting necessary parameters for batching, shuffling , and optimizing as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lODphjHX3STK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ceac75-1d62-488d-879d-e8b005cfe30c"
      },
      "source": [
        "batch_size = 64\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "buffer_size= train_ds.cardinality().numpy()\n",
        "\n",
        "train_ds = train_ds.shuffle(buffer_size=buffer_size)\\\n",
        "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
        "                   .cache()\\\n",
        "                   .prefetch(AUTOTUNE)\n",
        "\n",
        "test_ds = test_ds.shuffle(buffer_size=buffer_size)\\\n",
        "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
        "                   .cache()\\\n",
        "                   .prefetch(AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 17.7 ms (started: 2021-10-08 14:42:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R4pk6MkUWCJ",
        "outputId": "48064bff-a44c-4372-f5fd-5b8c6a3a95ab"
      },
      "source": [
        "train_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=<unknown>, dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(64,), dtype=tf.int64, name=None))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.28 ms (started: 2021-10-08 14:42:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQcMeCgkd4t6"
      },
      "source": [
        "# **PART D: BUILD AN END-TO-END MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD0PlXFiNSpa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67BIYNoP0E6v"
      },
      "source": [
        "# 12. Create a Classification Model\n",
        "\n",
        "For the sake of demonstration of the Keras `TextVectorization` layer, let's build a very simple model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2hK2ReMnHfz",
        "outputId": "b03c4fbb-ccee-420d-f9ae-cd7601311470"
      },
      "source": [
        "def create_model():\n",
        "    inputs_tokens = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    embedding_layer = layers.Embedding(vocab_size, 256)\n",
        "    x = embedding_layer(inputs_tokens)\n",
        "    x = layers.Flatten()(x)\n",
        "    outputs = layers.Dense(32)(x)\n",
        "    model = keras.Model(inputs=inputs_tokens, outputs=outputs)\n",
        "    \n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric_fn  = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    model.compile(optimizer=\"adam\", loss=loss_fn, metrics=metric_fn)  \n",
        "    \n",
        "    return model\n",
        "my_model=create_model()\n",
        "my_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 50)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 50, 256)           5120000   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12800)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                409632    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,529,632\n",
            "Trainable params: 5,529,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 54.3 ms (started: 2021-10-08 14:42:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGtGgE1FxsRb"
      },
      "source": [
        "# 13. Train the Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhiuaXoCxynm",
        "outputId": "725eb67b-65a5-4af6-a96d-10722326932b"
      },
      "source": [
        "my_model.fit(train_ds, verbose=1, epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "5391/5391 [==============================] - 251s 9ms/step - loss: 0.3019 - sparse_categorical_accuracy: 0.9310\n",
            "Epoch 2/3\n",
            "5391/5391 [==============================] - 46s 8ms/step - loss: 0.0424 - sparse_categorical_accuracy: 0.9897\n",
            "Epoch 3/3\n",
            "5391/5391 [==============================] - 46s 8ms/step - loss: 0.0049 - sparse_categorical_accuracy: 0.9993\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fef7fe05050>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6min 30s (started: 2021-10-08 14:42:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGjnwq6SyUX-",
        "outputId": "5e2ae4f4-2a7d-4b1c-d35f-e1ec2a680e21"
      },
      "source": [
        "loss, accuracy = my_model.evaluate(test_ds)\n",
        "print(\"Train accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1347/1347 [==============================] - 56s 3ms/step - loss: 0.2385 - sparse_categorical_accuracy: 0.9511\n",
            "Train accuracy:  0.9511414170265198\n",
            "time: 55.6 s (started: 2021-10-08 14:49:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zlRPwlBx-L_"
      },
      "source": [
        "# 14. An End-To-End Classification Model\n",
        "\n",
        "Pay attention that the above model is expected to receive batches of integer tensors as input:\n",
        "\n",
        "```\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " input_3 (InputLayer)        [(None, 50)]              0         \n",
        "```\n",
        "Thus, you can NOT supply raw data (some text) to the model for prediction. TensorFlow/Keras would generate error message as below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "budX5NPuMOS5"
      },
      "source": [
        "```python\n",
        "raw_data=['Dün aldığım samsung telefon bugün şarj tutmuyor',\n",
        "          'THY Uçak biletimi değiştirmek için başvurdum.  Kimse geri dönüş yapmadı!']\n",
        "\n",
        "predictions=my_model.predict(raw_data)\n",
        "\n",
        "ValueError: in user code: Exception encountered when calling layer \"model\" (type Functional).\n",
        "    \n",
        "    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1of input shape to have value 12800, but received input with shape (None, 256)\n",
        "    \n",
        "    Call arguments received:\n",
        "      • inputs=tf.Tensor(shape=(None,), dtype=string)\n",
        "      • training=False\n",
        "      • mask=None\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpxyclVcM4hB"
      },
      "source": [
        "However, sometimes it a big advantage if we can design a model which accepts raw data as input, then, process the data by itself.\n",
        "\n",
        "For example such a model can be easily exported different platforms/environments without the need of exporting the preprocess code!\n",
        "\n",
        "Therefore, Keras provides [several Preprocessing Layers](https://keras.io/api/layers/preprocessing_layers/) so that we can integrate preprocessing logic as a layer into a Keras model.\n",
        "\n",
        "After then, we can export such models and use any other platforms without re-writing preprocessing code on the exported platforms/environments.\n",
        "\n",
        "This kind of models can be called **End-To-End Models**. That is, an **End-To-End model** can accept Raw Input Data and preprocess it by itself.\n",
        "\n",
        "**What could be Raw Data? **\n",
        "\n",
        "It could be:\n",
        "* text\n",
        "* image\n",
        "* structure data\n",
        "* etc.\n",
        "\n",
        "Let's create an **End-To-End Classification Model** by integrating the **adapted** Keras `TextVectorization` layer into the **trained model** as **the first layer**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQgRdLHXL5jT"
      },
      "source": [
        "You can create an **End-To-End Model** either by:\n",
        "* Keras Sequential API, or\n",
        "* Keras Functional API "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyHOxb210keN"
      },
      "source": [
        "## 14.1. Create an End-To-End Model with Keras Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1IhG3stoIAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9098802-89f9-43fb-eab0-78b2d2bb18c9"
      },
      "source": [
        "end_to_end_model = tf.keras.Sequential([\n",
        "  keras.Input(shape=(1,), dtype=\"string\"),\n",
        "  vectorize_layer,\n",
        "  my_model,\n",
        "  layers.Activation('softmax')\n",
        "])\n",
        "\n",
        "end_to_end_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "end_to_end_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 50)               0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " model (Functional)          (None, 32)                5529632   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 32)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,529,632\n",
            "Trainable params: 5,529,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 282 ms (started: 2021-10-08 14:50:16 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeaGKzNN0o3Q"
      },
      "source": [
        "## 14.2. Create an End-To-End Model with Keras Functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkQNhhD8296z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54c72dd6-1558-4a00-819b-b90a5a083a8a"
      },
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorize_layer(inputs)\n",
        "outputs = my_model(x)\n",
        "end_to_end_model = keras.Model(inputs, outputs)\n",
        "end_to_end_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "end_to_end_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, 50)               0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " model (Functional)          (None, 32)                5529632   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,529,632\n",
            "Trainable params: 5,529,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "time: 284 ms (started: 2021-10-08 14:50:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw0C0IvF28T9"
      },
      "source": [
        "## 14.3. Test the End-to-End model with Raw (Text) Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmydvPGg3adQ",
        "outputId": "3be2e715-9e7a-47f5-b663-abc199a1f970"
      },
      "source": [
        "raw_data=['Dün aldığım samsung telefon bugün şarj tutmuyor',\n",
        "          'THY Uçak biletimi değiştirmek için başvurdum.  Kimse geri dönüş yapmadı!']\n",
        "predictions=end_to_end_model.predict(raw_data)\n",
        "print(id_to_category[np.argmax(predictions[0])])\n",
        "print(id_to_category[np.argmax(predictions[1])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alisveris\n",
            "ulasim\n",
            "time: 608 ms (started: 2021-10-08 14:50:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln_ydyp5yob6",
        "outputId": "584b3467-b163-452e-f8d8-640819f38fe3"
      },
      "source": [
        "loss, accuracy = end_to_end_model.evaluate(test_features,test_targets)\n",
        "print(\"end_to_end_model accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2696/2696 [==============================] - 47s 17ms/step - loss: 2.3769 - accuracy: 0.9511\n",
            "end_to_end_model accuracy:  0.9511488080024719\n",
            "time: 46.8 s (started: 2021-10-08 14:50:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhhQiICa7e38"
      },
      "source": [
        "## 14.4. Save the End-to-End model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEsPYR4e4Ihh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dca69c8-1677-458f-f8cb-f53407d2a964"
      },
      "source": [
        "end_to_end_model.save(\"end_to_end_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: end_to_end_model/assets\n",
            "time: 5.58 s (started: 2021-10-08 14:51:04 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KldU3Asf7kHV"
      },
      "source": [
        "## 14.5. Load the End-to-End model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLrdCADgng_D",
        "outputId": "d4d45ed0-cb15-494c-885a-bde17b1f1032"
      },
      "source": [
        "#changing the working directory\n",
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/models\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrKKxntwlHom"
      },
      "source": [
        "loaded_end_to_end_model = tf.keras.models.load_model(\"end_to_end_model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYFqCKDk7mxD"
      },
      "source": [
        "## 14.6. Test the Loaded End-to-End model with Raw (Text) Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgtRzOEg6led",
        "outputId": "3c3f223c-91d6-491a-d5f1-b54e9546a138"
      },
      "source": [
        "raw_data=['Dün aldığım samsung telefon bugün şarj tutmuyor',\n",
        "          'THY Uçak biletimi değiştirmek için başvurdum.  Kimse geri dönüş yapmadı!']\n",
        "predictions=loaded_end_to_end_model.predict(raw_data)\n",
        "print(id_to_category[np.argmax(predictions[0])])\n",
        "print(id_to_category[np.argmax(predictions[1])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alisveris\n",
            "ulasim\n",
            "time: 81.8 ms (started: 2021-12-03 14:12:35 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG0pj0Bv3LPW",
        "outputId": "115abd6c-d2c3-483d-cc12-952f1a5d776c"
      },
      "source": [
        "loss, accuracy = loaded_end_to_end_model.evaluate(test_features,test_targets)\n",
        "print(\"loaded_end_to_end_model accuracy: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2696/2696 [==============================] - 46s 17ms/step - loss: 2.3769 - accuracy: 0.9511\n",
            "loaded_end_to_end_model accuracy:  0.9511488080024719\n",
            "time: 46.1 s (started: 2021-10-08 14:51:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcfd6oqiD9R_"
      },
      "source": [
        "# **PART E: SUMMARY**\n",
        "In this tutorial, we have learned:\n",
        "* What a Keras `TextVectorization` layer is\n",
        "* Why we need to use a Keras `TextVectorization` layer in Natural Languge Processing (NLP) tasks\n",
        "* How to employ a Keras `TextVectorization` layer in Text Preprocessing\n",
        "* How to integrate a Keras `TextVectorization` layer to a trained model\n",
        "* How to save and upload a Keras `TextVectorization` layer and a model with a Keras `TextVectorization` layer\n",
        "* How to integrate a Keras `TextVectorization` layer with TensorFlow Data Pipeline API (`tf.data`)\n",
        "* How to design, train, save, and load an End-to-End model using Keras `TextVectorization` layer\n",
        "\n",
        "All above topics are presented in a **multi-class text classification** context.\n",
        "\n",
        "If you like this tutorial, please follow the Murat Karakaya Akademi [YouTube channel](https://www.youtube.com/c/MuratKarakayaAkademi) and [Medium blog](https://kmkarakaya.medium.com/).\n",
        "\n",
        "**Thank you for your patience!**\n",
        "\n",
        "#Keep Deep Learning :)"
      ]
    }
  ]
}